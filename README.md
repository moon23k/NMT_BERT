## NMT_BERT

Fine-Tuning large-scale pre-learning model represented by BERT to a specific task is effectively applied to various NLP tasks.
However, most of the applied tasks are simple tasks that add a top layer to the BERT, and are not applied relatively well to the Natural Language Generation Task of the encoder-decoder structure. Pointing out this problem, a paper named **'Incorporating BERT into Neural Machine Translation'** suggests a methodology using BERT for machine translation task. This repo, referring to the aforementioned paper, after applying BERT to NMT, compares the performance difference with that of the Transformer.

<br>
<br>

## Models
**NMT Transformer**

<br>

**NMT BERT**


<br>
<br>

## Configurations

<br>
<br>

## Result

<br>
<br>

## Reference
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/2002.06823)
* [Incorporating BERT into Neural Machine Translation](https://arxiv.org/abs/1810.04805)
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
